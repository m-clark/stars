# Polynomial Regression

## Using polynomials

A common application in regression to deal with nonlinear relationships involves polynomial regression.  For the predictor in question 

```{r datasetup, echo=FALSE}
set.seed(123)
x = rnorm(1000)
y = x - x^2 +.25*x^3 + rnorm(1000)
poly_dat = data.frame(x, y)

poly_dat %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25)) %>% 
  theme_plotly()
```

Let's fit a quadratic term and note the result.

```{r polymod, echo=1}
mod_poly = lm(y ~ poly(x, 2))

poly_dat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25), showlegend=F) %>% 
  add_lines(~x, ~pred, line=list(color="#03b3ff", opacity=1), name='fit') %>% 
  theme_plotly()
```

The R^2^ for this is `r broom::glance(mod_poly)$r.s %>% round(2)`, that's great! But look closely and you might notice that we aren't capturing the tails of this data at all.  Here's what the data and fit would look like if we extend the data, and things only get worse.

```{r poly_ack, echo=FALSE}
x2 = c(runif(250, -5, -2), runif(250, 2, 5))
y2 = x2 - x2^2 +.25*x2^3 + rnorm(500)
mod_gam = gam(y ~ s(x), data=poly_dat)
newdat = rbind(poly_dat, data.frame(x=x2, y=y2))
newdat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25), showlegend=F) %>% 
  add_lines(~x, ~pred, name='poly', line=list(color="#03b3ff", opacity=1)) %>% 
  theme_plotly()
```

Part of the reason is that, outside of deterministic relationships due known physical or other causes, you are unlikely to discover a quadratic relationship between variables among found data.  Even when it appears to fit, it is almost certainly overfit due to this reason.  Fitting a polynomial is more akin to enforcing our vision of how the data should be, rather than letting the data speak for itself. Sure, it might be a good approximation some of the time, just as assuming a linear relationship is, but often it's just wishful thinking.


Compare the previous result to the following fit from a generalized additive model.  GAMs are susceptible to extrapolation as is every statistical model ever created.  However, the original fit (in red) is much better. Notice how it was better able to follow the straightened out data points at the high end, rather than continuing the bend the quadratic enforced.

```{r polyplusgam, echo=FALSE}
newdat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.1), showlegend=F) %>% 
  add_lines(~x, ~pred, name='poly', line=list(color='#03b3ff')) %>% 
  add_lines(~x, ~pred, name='gam',  line=list(color='#a703ff'), data=add_predictions(newdat, mod_gam)) %>% 
  add_lines(~x, ~pred, name='gam',  line=list(color='#ff1803'), data=add_predictions(poly_dat, mod_gam)) %>% 
  theme_plotly()
```


## A more motivating example

Perhaps you would have been satisfied with the initial quadratic fit above or perhaps a cubic fit[^cubic]. We may come across a situation where the target of interest $y$ is a function of some covariate $x$, whose effect is not straightforward at all.  In a standard regression we can generally write the model as follows:

$$y = f(x) + e$$

If we are talking a linear relationship between y and x, f(x) might be a simple sum of the covariates.

$$y = b_0 + b_1*x + e$$

In that case we could do our standard regression model.  Now consider the following functional form for x:

$$f(x) = sin(2(4x-2)) + 2e^{-(16^2)(x-.5)^2} + \epsilon$$
$$\epsilon \sim N(0,.3^2)$$

```{r simData}
set.seed(123)
x = runif(500)
mu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))
y = rnorm(500, mu, .3)
d = data.frame(x,y) 
```

Let's take a look at it visually.

```{r simDataPlot, echo=F}
#ff55035 = alpha('#ff5503', .05)   # apparently will only work with I(#ff550325)
#ff550325 = alpha('#ff5503', .25)
plot_ly(width=700, data=d) %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.5)) %>% 
  theme_plotly() %>% 
  layout()
```



### Polynomial regression doesn't work

As above we would try and use polynomial regression, e.g. fitting a quadratic or cubic function within the standard regression framework.  However, this is unrealistic at best and at worst isn't useful for complex relationships. In the following even with a polynomial of degree 15 the fit is fairly poor in many areas.

```{r polyreg, echo=FALSE}
fits = sapply(seq(3,15, 3), function(p) fitted(lm(y~poly(x,p)))) %>% 
  data.frame(x, y, .) %>% 
  gather(key=polynomial, value=fits, -x, -y) %>% 
  mutate(polynomial = factor(polynomial, labels=seq(3,15, 3)))


plot_ly(width=700, data=d) %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.1), showlegend=F) %>% 
  add_lines(~x, ~fits, color=~polynomial, data=fits) %>% 
  theme_plotly()
```


It's maybe also obvious no transformation is going to help us either in this case.  In general, we'll need tools better suited to more complex relationships, or simply ones that don't require us to overfit/simplify the relationship we see.

[^cubic]: The data was actually generated with a cubic polynomial.