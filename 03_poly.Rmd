# Polynomial Regression

## Using polynomials

A common application in regression to deal with nonlinear relationships involves polynomial regression.  For the predictor in question, $x$, we add terms e.g. quadratic ($x^2$), cubic ($x^3$) etc. to get a better fit.  Consider the following data situation.

```{r datasetup, echo=FALSE}
set.seed(123)
x = rnorm(1000)
y = x - x^2 +.25*x^3 + rnorm(1000)
poly_dat = data.frame(x, y)

poly_dat %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25)) %>% 
  theme_plotly()
```

Let's fit a quadratic term and note the result.

```{r polymod, echo=1}
mod_poly = lm(y ~ poly(x, 2))

poly_dat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25), showlegend=F) %>% 
  add_lines(~x, ~pred, line=list(color="#03b3ff", opacity=1), name='fit') %>% 
  theme_plotly()
```

The R^2^ for this is `r broom::glance(mod_poly)$r.s %>% round(2)`, that's great! But look closely and you might notice that we aren't capturing the tails of this data at all.  Here's what the data and fit would look like if we extend the data based on the underlying true function, and things only get worse.

```{r poly_ack, echo=FALSE}
x2 = c(runif(250, -5, -2), runif(250, 2, 5))
y2 = x2 - x2^2 +.25*x2^3 + rnorm(500)
mod_gam = gam(y ~ s(x), data=poly_dat)
newdat = rbind(poly_dat, data.frame(x=x2, y=y2))
newdat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.25), showlegend=F) %>% 
  add_lines(~x, ~pred, name='poly', line=list(color="#03b3ff", opacity=1)) %>% 
  theme_plotly()
```

Part of the reason is that, outside of deterministic relationships due known physical or other causes, you are unlikely to discover a quadratic relationship between variables among found data.  Even when it appears to fit, without a lot of data it is almost certainly <span class="emph">overfit</span> due to this reason.  Fitting a polynomial is more akin to enforcing our vision of how the data should be, rather than letting the data speak for itself. Sure, it might be a good approximation some of the time, just as assuming a linear relationship is, but often it's just wishful thinking.


Compare the previous result to the following fit from a generalized additive model.  GAMs are susceptible to extrapolation, as is every statistical model ever created.  However, the original fit (in red) is much better. Notice how it was better able to follow the straightened out data points at the high end, rather than continuing the bend the quadratic enforced.

```{r polyplusgam, echo=FALSE}
newdat %>% 
  add_predictions(mod_poly) %>% 
  plot_ly() %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.1), showlegend=F) %>% 
  add_lines(~x, ~pred, name='poly', line=list(color='#03b3ff')) %>% 
  add_lines(~x, ~pred, name='gam',  line=list(color='#a703ff'), data=add_predictions(newdat, mod_gam)) %>% 
  add_lines(~x, ~pred, name='gam',  line=list(color='#ff1803'), data=add_predictions(poly_dat, mod_gam)) %>% 
  theme_plotly()
```


## A more complex relationship

Perhaps you would have been satisfied with the initial quadratic fit above or perhaps a cubic fit[^cubic]. We may come across a situation where the target of interest $y$ is a function of some covariate $x$, whose effect is not straightforward at all.  In a standard regression we can generally write the model as follows:

$$y = f(x) + e$$

If we are talking a linear relationship between y and x, f(x) might be a simple sum of the covariates.

$$y = b_0 + b_1*x + e$$

In that case we could do our standard regression model.  Now consider the following functional form for x:

$$f(x) = sin(2(4x-2)) + 2e^{-(16^2)(x-.5)^2} + \epsilon$$
$$\epsilon \sim N(0,.3^2)$$

Let's generate some data and take a look at it visually.

```{r simData}
set.seed(123)
x = runif(500)
mu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))
y = rnorm(500, mu, .3)
d = data.frame(x,y) 
```



```{r simDataPlot, echo=F}
plot_ly(width=700, data=d) %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.5)) %>% 
  theme_plotly() %>% 
  layout()
```



### Polynomial regression is problematic

As above, we could try and use polynomial regression here, e.g. fitting a quadratic or cubic function within the standard regression framework.  However, this is unrealistic at best and at worst isn't useful for complex relationships. In the following even with a polynomial of degree 15 the fit is fairly poor in many areas.

```{r polyreg, echo=FALSE}
fits = sapply(seq(3,15, 3), function(p) fitted(lm(y~poly(x,p)))) %>% 
  data.frame(x, y, .) %>% 
  gather(key=polynomial, value=fits, -x, -y) %>% 
  mutate(polynomial = factor(polynomial, labels=seq(3,15, 3)))


plot_ly(width=700, data=d) %>% 
  add_markers(~x, ~y, marker=list(color='#ff5503', opacity=.1), showlegend=F) %>% 
  add_lines(~x, ~fits, color=~polynomial, data=fits) %>% 
  theme_plotly()
```


It's maybe also obvious that a target transformation is going to help us in this case.  In general, we'll need tools better suited to more complex relationships, or simply ones that don't require us to overfit/simplify the relationship we see, or guess about the form randomly until finding a decent fit.

[^cubic]: The data was actually generated with a cubic polynomial.