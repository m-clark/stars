# Practical GAM

One of the more powerful modeling packages in R comes with its installation[^gampack].  The mixed gam computational vehicle, or <span class="pack">mgcv</span> package, is an extremely flexible modeling tool, and one we'll use for our foray into generalized additive models.  If you're familiar with standard regression models, the syntax is little different, but adds a great many possibilities to your efforts.


## Getting started


So GAMs can be seen as a special type of GLM, that extends the model to incorporate nonlinear and other relationships, but while keeping things within a familiar framework.  Behind the scenes, extra columns are added to our model matrix to do this, but their associated coefficients are penalized to help avoid overfitting.

As we will see, the main syntactical difference between using <span class="func">gam</span> and using <span class="func">glm</span> is that you'll use the <span class="func">s</span> function to incorporate <span class="emph">smooth terms</span>, or classes that include basis functions as we saw before, along with a penalty.  Beyond what we did previously, <span class="pack">mgcv</span> also incorporates a penalized regression approach.  For those familiar with lasso, ridge or other penalized approaches you'll find the same idea here.  For those new to this, it's important to add the concept and technique to your statistical modeling toolbox.

Let's look at some more interesting data. For the following example we'll use the iris data set.  Just kidding! We'll look at some data regarding the Internet Movie Database (IMDB) from the <span class="pack">ggplot2movies</span> package. If you want to look at the original, you'll need to install the package, and the data object within is called <span class="objclass">movies</span>.  For for information, type `?movies` after loading the package.  

However, our example will concern yearly averages based on that data set. The primary data set is `movies_yearly`, with a film budget variable that has been adjusted to 2016 dollars. In addition, I've grouped them by whether they are an action movie or not in a separate data set `movies_yearly_action`. And finally, the data has been filtered to contain only those years where there a decent number of movies to go into the calculations (10 or more for yearly, 5 or more for yearly_action), and feature films (length in minutes > 60).


```{r movies_data_setup, eval=FALSE, echo=FALSE, cache=TRUE}
library(ggplot2movies)
library(lubridate)
monthly_cpi = read_csv('data/CPIAUCSL.csv') # from https://fred.stlouisfed.org/
monthly_cpi$year = year(monthly_cpi$DATE)

yearly_cpi = monthly_cpi %>%
  mutate(year = year(DATE)) %>% 
  group_by(year) %>%
  summarize(cpi = mean(CPIAUCSL)) %>% 
  mutate(adj_factor = cpi/cpi[year == 2016])

movies2 = left_join(movies, yearly_cpi) %>% 
  # filter(!is.na(budget)) %>% 
  mutate(budget_2016 = budget/adj_factor,
         budget_2016 = if_else(title=="Voyna i mir", 10000000/adj_factor, budget_2016))  # correction see https://en.wikipedia.org/wiki/List_of_most_expensive_films#Most_expensive_films_.28adjusted_for_inflation.29

movies_yearly_action = movies2 %>% 
  filter(length>60) %>% #length>60,
  mutate(Action=factor(Action, labels=c('Other', 'Action'))) %>% 
  group_by(year, Action) %>%
  mutate(N=n()) %>% 
  summarise_at(.cols = vars(budget_2016, rating, votes, length, N), 
               .funs = funs('_'= round(mean(., na.rm=T), digits=1))) %>% # unclear why renaming variables based on function is REQUIRED
  rename(budget_2016=budget_2016__,
         rating=rating__,
         votes=votes__,
         length=length__,
         N=N__) %>% 
  mutate(budget_2016 = budget_2016/1e6,
         votes = votes/1000,
         length = length/60) %>% 
  ungroup() %>% 
  filter(N>5)


movies_yearly = movies2 %>% 
  filter(length>=60) %>% #length>60,
  group_by(year) %>% 
  mutate(N=n()) %>% 
  summarise_at(.cols = vars(budget_2016, rating, votes, length,N), 
               .funs = funs('_'= round(mean(., na.rm=T), digits=1))) %>% 
  rename(budget_2016=budget_2016__,
         rating=rating__,
         votes=votes__,
         length=length__,
         N=N__) %>% 
  mutate(budget_2016 = budget_2016/1e6,
         votes = votes/1000,
         length = length/60) %>% 
  ungroup() %>% 
  filter(N>10)

save(movies2, movies_yearly, movies_yearly_action, file='data/movies_yearly.RData')


# lake data from http://www.lre.usace.army.mil/Missions/Great-Lakes-Information/Great-Lakes-Water-Levels/Historical-Data/
lakes = read_csv('data/GLHYD_data_metric.csv') %>% 
  mutate(month_num = rep(1:12, n_distinct(year)),
         month = factor(month)) %>% 
  gather(key=lake, value=meters, -month, -month_num, -year, factor_key=T) %>% 
  mutate(lake = relevel(lake, 'Ontario'))
# lakes = read_csv('data/GLHYD_data_english.csv') %>% 
#   mutate(month_num = rep(1:12, n_distinct(year))) %>% 
#   gather(key=lake, value=meters, -month, -month_num, -year, factor_key=T)

lakes_yearly = lakes %>%
  group_by(year, lake) %>% 
  summarise(meanHeight=mean(meters))

save(lakes, lakes_yearly, file='data/greatLakes.RData')


```


```{r readimdb, echo=1:2}
library(ggplot2movies)
load(file='data/movies_yearly.RData')
DT::datatable(movies_yearly, options=list(dom='pt', scrollX=T, scrollY=T), width='100%')
```

<br>
Let's examine the the time trend for ratings and budget. Size of the points represents the number of movies that year.
<br>

```{r rating_trend, echo=F}
movies_yearly %>% 
  plot_ly() %>% 
  add_markers(~year, ~rating, color=I('#ff5503'), size=~N, showlegend=F, name='') %>% 
  add_lines(~year, ~rating, color=I('#ff5503'), showlegend=F) %>% 
  layout(title = "Yearly Trend for Ratings", 
         xaxis = list(title = "Year", showgrid = F),
         yaxis = list(title = "Average Rating")) %>% 
  theme_plotly()
```

For ratings there seems to be a definite curvilinear trend, where initially ratings for movies started high and decreased, but were on the upswing at later years.

Adjusted for inflation, movie budgets don't increase linearly over time as one might expect, and in fact were decreasing towards the end. Note that we only have *adjusted* budget data for 1948 on.


```{r budget_trend, echo=F}
movies_yearly %>% 
  plot_ly() %>% 
  add_markers(~year, ~budget_2016, color=I('#ff5503'), size=~N, showlegend=F, name='') %>% 
  add_lines(~year, ~budget_2016, color=I('#ff5503'), showlegend=F) %>% 
  layout(title = "Yearly Trend for Budgets", 
         xaxis = list(title = "Year", showgrid = F),
         yaxis = list(title = "Average Budget in Millions")) %>% 
  theme_plotly()
```

## Fitting the model

Let's fit a GAM to the yearly trend for ratings.  I will also put in budget (in millions), movie length (in hours), and number of votes (in thousands) as standard effects.  We'll use a cubic regression spline for our basis, `bs='cr'`, but aside from specifying the smooth term, the syntax is the same as one would use for the <span class="func">glm</span> function.
<br>

```{r gam_rating}
library(mgcv)
gam_model = gam(rating ~ s(year, bs='cr') + budget_2016 + votes + length, 
                data=movies_yearly)
summary(gam_model)
```

The first part of our output contains standard linear regression results. It is important to note that there is nothing going on here that you haven't seen in any other GLM. For example, in this case there is no relationship between budget and rating, whereas adding an hour to the film might bump up the rating around near a point[^longmovie].

```{r parametric, echo=FALSE}
cat("Parametric coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  4.336424   0.945088   4.588 3.35e-05 ***
budget_2016 -0.001436   0.003166  -0.453    0.652    
votes        0.046273   0.121571   0.381    0.705    
length       0.885090   0.586945   1.508    0.138     ")
```


The next part contains our information regarding the smooth terms, of which there is only one.


```{r nonparametric, echo=FALSE}
cat("Approximate significance of smooth terms:
          edf Ref.df     F p-value    
s(year) 7.136  8.129 28.88  <2e-16 ***")
```

While this suggests a statistically significant effect, it's important to note there is not a precisely defined p-value in this setting- it clearly states 'approximate significance'.  For some more details see the [technical section][interpreting output for smooth terms] or [my document](https://m-clark.github.io/docs/GAM.html), but for now, we'll  start with the <span class="emph">effective degrees of freedom</span>, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but they are each penalized to some extent and thus the *effective* degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df). 

The thing to note is that the edf would equal 1 if the model penalized the smooth term to a simple linear relationship[^allthewaytozero], and so the effective degrees of freedom falls somewhere between 1 and k-1, where k is chosen based on the basis. You can think of it as akin to the number of knots.  The default here was 10. Basically if it's 1, just leave it as a linear component, or if it's close to k, maybe bump up the default to allow for more wiggliness.


### Visualizing the effects

While the statistical result is helpful to some extent, the best way to interpret smooth terms is visually.  The ability to do so is easy, you can just use the <span class="func">plot</span> method on the model object. However, the default plot may be difficult to grasp at first.  It is a [component plot](https://en.wikipedia.org/wiki/Partial_residual_plot#CCPR_plot), which plots the original variable against the linear combination of its basis functions, i.e. the sum of each basis function multiplied by its respective coefficient. For example, if we were using a quadratic polynomial, it would be the plot of $x$ against $b_1\cdot x + b_2\cdot x^2$. For GAMs, $y$ is also centered, and what you end up with is something like the following.

```{r gamplot, dev='svglite'}
plot(gam_model)
```

So this tells us the contribution of the year to the model fit. For an alternative, consider the <span class="pack">visreg</span> package, which will put it back on the original scale, and just look slightly nicer by default while allowing for more control.

```{r visreg, dev='svglite'}
library(visreg)
visreg(gam_model, xvar='year', partial=F)
```

Again, we're see the fit after partialing out the effects of budget, number of votes, and length.

You of course can look at predictions at any relevant values for the covariates, as you can with any other model. Consider the following effect of time while keeping budget and length constant at their means, and votes at 1000[^atmeans].

```{r gam_predict}
movies_yearly %>% 
  na.omit() %>%
  transmute(budget_2016 = mean(budget_2016),
            length = mean(length),
            votes = 1,
            year=year) %>% 
  add_predictions(gam_model, var='Predicted_Rating') %>% 
  plot_ly() %>% 
  add_lines(~year, ~Predicted_Rating)
```

In the end, controlling for budget, length, and popularity, movie quality decreased notably until the 70s, and remained at that level until the late 90s, when quality increased even more dramatically.








[^gampack]: For reasons I've not understood, other packages still depend on or extend the <span class="pack">gam</span> package, which doesn't possess anywhere near the functionality (though the authors literally wrote the book on GAM).


[^longmovie]: Presumably we have to justify the time spent watching longer movies. Note also I personally consider the length effect significant, despite the p-value.

[^allthewaytozero]: You can actually penalize the covariate right out of the model if desired, i.e. edf=0.

[^atmeans]: If we had put votes at its mean, you would basically duplicate the previous plot.
