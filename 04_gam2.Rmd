# Practical GAM

One of the more powerful modeling packages in R comes with its installation[^gampack].  The mixed gam computational vehical, or <span class="pack">mgcv</span> pacakage, is an extremely flexible modeling tool, and one we'll use for our foray into generalized additive models.  If you're familiar with standard regression models, the syntax is little different, but adds a great many possibilities to your efforts.


## Getting started


```{r echo=FALSE, eval=2}
# some initial exploration
imdb = read.csv('data/movie_metadata.csv')
# note this data has extra characters and possibly other issues
imdb %>% summary
imdb %>%
  filter(country=='USA', budget>1e6) %>%
  # filter(num_voted_users>50000, budget>median(budget, na.rm=T), country=='USA') %>% 
  qplot(x=title_year, y=budget, data=.) +
  # qplot(x=budget, y=imdb_score, data=.) +
  # qplot(x=gross, y=imdb_score, data=.) +
  # geom_point(aes(color=movie_title), show.legend=F) +
  geom_smooth()


# create big budget
imdb %>%
  filter(country=='USA', budget>1e8) %>%
  mutate(movie_title=stringr::str_replace_all(movie_title, pattern='Â', replacement=''),  # unfortunately this doesn't fix that pattern
         movie_title=rvest::repair_encoding(movie_title),
         movie_title=stringr::str_sub(movie_title, end=-2),
         budget_in_millions = budget/1e6) %>% 
  write_csv('data/bigbudget.csv')

# create small budget
imdb %>%
  filter(country=='USA', budget<1e6) %>%
  mutate(movie_title=stringr::str_replace_all(movie_title, pattern='Â', replacement=''),  # unfortunately this doesn't fix that pattern
         movie_title=rvest::repair_encoding(movie_title),
         movie_title=stringr::str_sub(movie_title, end=-2)) %>% 
  write_csv('data/smallbudget.csv')

imdb %>%
  filter(country=='USA', budget>1e8) %>%
  mutate(movie_title=stringr::str_replace_all(movie_title, pattern='Â', replacement='')) %>% 
  plot_ly() %>% 
  add_markers(x=~gross, y=~imdb_score, text = ~movie_title) %>% 
  lazerhawk::theme_plotly()

imdb %>%
  filter(country=='USA', budget>1e8) %>%
  # filter(country=='USA', duration>60) %>%
  mutate(movie_title=stringr::str_replace_all(movie_title, pattern='Â', replacement='')) %>% 
  plot_ly() %>% 
  add_markers(x=~gross, y=~imdb_score, text = ~movie_title, color=I('#ff5503')) %>%
  # add_markers(x=~duration, y=~gross, text = ~movie_title, marker=list(color='#ff5503', opacity=.25)) %>% # interesting example for mixture
  lazerhawk::theme_plotly()

plot(gam(imdb_score ~ s(gross) + title_year + duration, data=imdb))
imdb %>% 
  select_if(is.numeric) %>% 
  cor(use='pair') %>% 
  heatR::corrheat()

# imdb_small %>%
#   filter(num_voted_users>10000) %>% 
#   plot_ly() %>% 
#   add_markers(x=~budget, y=~imdb_score, text = ~movie_title, color=I('#ff5503')) %>%
#   lazerhawk::theme_plotly()
# imdb_small %>%
#   filter(num_voted_users>10000) %>%
#   ggplot(aes(x=budget, y=imdb_score)) +
#   geom_point() +
#   geom_smooth()
imdb_big %>% 
  group_by(title_year) %>% 
  summarize(AvgRating=mean(imdb_score)) %>% 
  qplot(data=., title_year, AvgRating, geom='point')
```

So GAMs can be seen as a special type of GLM, that extends the model to incorporate nonlinear and other relationships.  Behind the scenes, extra columns are added to our model matrix to do this, but their associated coefficients are penalized to help avoid overfitting.

As we will see, the main syntactical difference between using <span class="func">gam</span> and using <span class="func">glm</span> is that you'll use the <span class="func">s</span> function to incorporate <span class="emph">smooth terms</span>, or classes that include basis functions as we saw before, along with a penalty.  Beyond what we did previously, mgcv also incorporates a penalized regression approach.  For those familiar with lasso, ridge or other penalized approaches you'll find the same idea here[^penalty].  For those new to this, it's important to add the concept and technique to your statistcal modeling toolbox.

Let's look at some more interesting data. For the following example we'll use the iris data set.  Just kidding! We'll look at some data regarding the Internet Movie Database (IMDB) from the <span class="pack">ggplot2movies</span> package (you'll need to install it, the data object is called 'movies').


<!-- We'll look at some data regarding the Internet Movie Database (IMDB) available from [Kaggle](https://www.kaggle.com/datasets) (now a Google product).  I have the whole data available in the repo for this document, but we'll use the 'big budget' subset, which includes only movies made in the U.S. that cost at least 100 million.  The data contains things like the year of the movie, the rating, number of reviews, budget, gross, etc.[^imdb] -->


```{r datasetup, eval=FALSE, echo=FALSE}
library(ggplot2movies)
library(lubridate)
monthly_cpi = read.csv("http://research.stlouisfed.org/fred2/data/CPIAUCSL.csv", header = TRUE)
monthly_cpi$year = year(monthly_cpi$DATE)
yearly_cpi = monthly_cpi %>%
  group_by(year) %>%
  summarize(cpi = mean(VALUE)) %>% 
  mutate(adj_factor =cpi/cpi[year == 2016])
movies = left_join(movies, yearly_cpi) %>% 
  filter(!is.na(budget)) %>% 
  mutate(budget_cpi = budget/adj_factor)

movies %>% 
  select(title, year, budget, budget_cpi) %>% 
  head(50)
# note voyna i mir, i.e. war and peace is incorrect on the imdb website; should be 10 mil, not 100
movies %>% 
  filter(budget>1000000) %>% 
  plot_ly() %>% 
  add_markers(~budget_cpi, ~rating, size=~budget_cpi, text=~title)
```


```{r readimdb, echo=1}
library(ggplot2movies)
# imdb_big = read.csv('data/bigbudget.csv')
# imdb_small = read.csv('data/smallbudget.csv')

DT::datatable(head(movies, 100), options=list(dom='t', scrollX=T, scrollY=T), width='100%')
```

<br>
Let's examine the relationship between the number of votes as a proxy for popularity, and it's rating.  For a little bit more manageable data, we'll use yearly means. 
<br>



```{r plot_movies, eval=1, echo=1}

# ?movies
movies = movies %>% 
  filter(votes > 1000) %>% 
  group_by(year) %>% 
  
  
  
  
movies %>%
  plot_ly() %>% 
  add_markers(x=~votes, y=~rating, text = ~title, color=I('#ff5503')) %>%
  lazerhawk::theme_plotly()
movies %>%
  filter(budget>1e6) %>% 
  plot_ly() %>% 
  add_markers(x=~year, y=~rating, text = ~title, color=I('#ff5503')) %>%
  lazerhawk::theme_plotly()
movies %>%
  filter(votes > 1000) %>% 
  plot_ly() %>% 
  add_markers(x=~year, y=~rating, text = ~title, color=I('#ff6600')) %>%
  lazerhawk::theme_plotly()
# imdb_big %>%
#   plot_ly() %>% 
#   add_markers(x=~num_voted_users, y=~imdb_score, text = ~movie_title, color=I('#ff5503')) %>%
#   lazerhawk::theme_plotly()
# imdb_big %>%
#   plot_ly() %>% 
#   add_markers(x=~title_year, y=~imdb_score, text = ~movie_title, color=I('#ff5503')) %>%
#   lazerhawk::theme_plotly()

```

<br>
For these big budget movies, having more votes is associated with higher ratings, but less so with increasing popularity.  Let's fit a gam to this.  I will also put in budget (in millions) and year as standard effects.  We'll use a cubic regression spline for our basis, `bs='cr'`.
<br>

```{r}
library(mgcv)
gam_model = gam(imdb_score ~ title_year + budget_in_millions + s(num_voted_users, bs='cr'), 
                data=imdb_big)
summary(gam_model)
```

The first part of our output contains standard linear regression results. It is important to note that there is nothing going on here that you haven't seen in any other GLM. In this case there is no relationship between budget and score once you've already spent this much in the first place.  It does look like scores increase slightly over time, but there is only about a .2 rating difference after 10 years.

```{r parametric, echo=FALSE}
cat("Parametric coefficients:
                     Estimate Std. Error t value Pr(>|t|)   
(Intercept)        -37.963762  16.347639  -2.322  0.02099 * 
title_year           0.022265   0.008163   2.728  0.00682 **
budget_in_millions  -0.000665   0.001164  -0.571  0.56819   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1")
```


The next part contains our information regarding the smooth terms, of which there is only one.


```{r nonparametric, echo=FALSE}
cat("Approximate significance of smooth terms:
                     edf Ref.df     F p-value    
s(num_voted_users) 2.356  2.842 87.39  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1")
```

However, the best way to interpret the smooth terms is visually.  The ability to so is easy, but the default plot may be difficult to grasp at first.  It is a component plot, which plots the original variable against the linear combination of its basis functions, i.e. the sum of each basis function multiplied by its respective coefficient. For example, if we were using a quadratic polynomial, it would be the plot of x against $b_1\cdot x + b_2\cdot x^2$. For GAMs, y is also centered, and what you end up with is something like the following.

```{r gamplot}
# y = scale(imdb_big)
```





We'll start with the effective degrees of freedom, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but they are each penalized to some extent and thus the *effective* degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df).  For more on this see my document, `?summary.gam` and `?anova.gam`. 

At this point you might be thinking these p-values are a bit fuzzy, and you'd be right.   As is the case with mixed models, machine learning approaches, etc. p-values are not straightforward. The gist is, they aren't to be used for harsh cutoffs, say, at an arbitrary .05 level, but then standard p-values shouldn't be used that way either.  If they are pretty low you can feel comfortable claiming statistical significance, but if you want a tight p-value you'll need to go back to using a glm. 

The edf would equal 1 if the model penalized the smooth term to a simple linear relationship[^allthewaytozero], and so the effective degrees of freedom falls somewhere betweeen 1 and k, where k is chosen based on the basis. You can think of it as akin to the number of knots.  The default here was 10.  There is functionality to choose the k value, but note:


> So, exact choice of k is not generally critical: it should be chosen to be large enough that you are reasonably sure of having enough degrees of freedom to represent the underlying 'truth' reasonably well, but small enough to maintain reasonable computational efficiency. Clearly 'large' and 'small' are dependent on the particular problem being addressed.



For the standard gaussian setting, we can use our R^2^.  Also provided is 'deviance explained', which in this setting is identical, but for non-gaussian families might be preferred. The GCV, or generalized cross validation score, can be taken as an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. 

```{r rsq, echo=FALSE}
cat("R-sq.(adj) =   0.51   Deviance explained = 51.8%
GCV = 0.44245  Scale est. = 0.43351   n = 265")
```



[^gampack]: For reasons I've not understood, other packages still depend on or extend the <span class="pack">gam</span> package, which doesn't possess anywhere near the functionality (though the authors literally wrote the book on GAM).

[^penalty]: a quadratic penalty specifically

[^imdb]: One might think this would be more interesting data, but there is actually not much variability in scores.  They have a mean of `r round(mean(imdb$imdb_score), 1)` and standard deviation, of `r round(sd(imdb$imdb_score), 1)`, with some skew (notable duds).  Other variables are notable rounded.

[^allthewaytozero]: You can actually penalize the covariate right out of the model if desired, i.e. edf=0.