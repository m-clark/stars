[
["index.html", "My God! It’s full of STARs! Using Astrology to Enhance Your Results!", " My God! It’s full of STARs! Using Astrology to Enhance Your Results! Michael Clark https://m-clark.github.io/ 2018-08-30 "],
["01_intro.html", "Preface Audience Prerequisites Goal Outline Workshop Setup Other", " Preface This talk/workshop provides a brief introduction to generalized additive models (GAMs), or even more generally, or structured additive regression models (STARs). They offer a natural extension to common generalized linear models, while having the flexibility to incorporate a wide range of modeling situations. Audience Specifically, I have in mind applied researchers/students with a burgeoning interest in using models they probably weren’t exposed to in their typical statistical training. More generally, anyone that finds it useful. Prerequisites Prerequisites include a firm understanding of linear regression and exposure to generalized linear models. Some experience with using R for statistical modeling is required to follow along with the code and do the exercises. The workshop will potentially see a wide range of skill sets and modeling experience among attendees, but aims to be accessible to all. As such, the practical is favored over the technical, but some details are provided as an appendix, with additional references for those interested in going deeper down the rabbit hole. Goal To provide to those attending an overview of generalized additive models and their extensions. In addition, I hope to provide enough familiarity with what such models offer, via conceptual examples and exercises, that those attending will feel able to consider them in future modeling endeavors. Outline Why demo where lm fails traditional ways of dealing with problems that arise Transition: standard polynomial where it works where it isn’t applicable/fails Introducing GAMs basic info getting started Note Issues things to get used to Extensions &amp; Ties to other Additional choices for smooth terms Random effects Spatial GP Boosting/RF Technicals by-hand example penalized regression Workshop Setup The goal of the following is to create an RStudio project (i.e. a folder that represents the working directory moving forward) with a folder inside called ‘data’ that has the data for the demonstrations and exercises. For those not used to RStudio projects, read on, and use them for everything you do from now on. RStudio Click File/New Project/New Directory and give it a name, e.g. GAM_Workshop. BE MINDFUL ABOUT WHERE YOU ARE CREATING IT. You don’t have to do anything else with RStudio at this point, but leave it open. Data The data used in the document demonstrations and exercises is available at https://m-clark.github.io/workshops/stars/data.7z. It is a zipped archive containing several data files and .RData objects. You must: Download it. BE INTENTIONAL ABOUT WHERE YOU DOWNLOAD IT. Unzip it. Note that this is a different step than number 1, and failure to do this will render the stuff inside inaccessible. BE PURPOSEFUL ABOUT WHERE YOU UNZIP IT. Copy/Move it. This now usable folder needs to go into your project folder that you just created. This can actually be done as part of step 2. When you are finished, you will now have an R project folder with a data folder inside it which contains the example data sets. To demonstrate this, in RStudio you should be able to open a script and run load('data/movies_yearly.RData'). Other If you’re interested in the underlying code, visuals, markdown etc. these notes have a GitHub repository at https://github.com/m-clark/stars. "],
["02_lm.html", "Why not just use standard methods? Heteroscedasticity, non-normality etc.", " Why not just use standard methods? The standard linear model is ubiquitous in statistical training and application, and for good reason. It is simple to do and easy to understand. Let’s go ahead and do one to get things started. mod = lm(y ~ x1 + x2, data=dat) summary(mod) term estimate std.error statistic p.value (Intercept) 7.27 0.33 21.83 0 x1 6.36 0.44 14.62 0 x2 -5.12 0.45 -11.31 0 r.squared value 0.46 Everything is nice and tidy. We have straightforward information, positive effect of x1, negative for x2, and familiar output. Depending on your context, the R2 may or may not be something exciting. Let’s look at some diagnostics1. Some issues might be present, as we might be getting a little more variance with some, especially higher, fitted values. We’re also a little loose in the tails of the distribution of the residuals. Let’s compare our predictions to the data. With a strong model we might see a cigar shaped cloud converging to a line with slope 1 as the fit gets better. We seem to be having some issues here, as the residual plot noted above. Now let’s go back and visualize the data. The following plots both predictors against the target variable. Yikes. We certainly have a positive effect for x1, but it looks rather curvy. The other predictor doesn’t appear to have a relationship that could be classified as easily. It is sharply positive for low values of x2, but negative thereafter, though not in a straightforward fashion. Heteroscedasticity, non-normality etc. In many cases as above, people have some standby methods for dealing with the problem. For example, they might see the qqplot for the residuals and think some of those cases are ‘outliers’, perhaps even dropping them from analysis. Others might try a transformation of the target variable, for example, in cases of heteroscedasticity (not because of non-normality!) some might take the log. modlog = lm(log(y) ~ x1 + x2, dat) summary(modlog) term estimate std.error statistic p.value (Intercept) 1.83 0.05 33.84 0 x1 0.94 0.07 13.29 0 x2 -0.69 0.07 -9.37 0 r.squared value 0.4 Well, our fit in terms of R2 has actually gone down. Let’s check the diagnostics. The transformation may have helped in some ways, but made other things worse. We continue to see some poor fitting cases and now our fit is flattening even more than it was. This is a fairly typical result. Transformations often exacerbate data issues or fail to help. What’s more, some of them lead to more difficult interpretation, or aren’t even applicable (e.g. categorical, ordinal targets). Outliers, if there was actually a standard for deeming something as such, are just indications that your model doesn’t capture the data generating process in some fashion. Cutting data out of the modeling process for that reason hasn’t been acceptable for a long time (if it ever was). Data abounds where a standard linear model performs poorly or doesn’t do a good enough job capturing the nuances of the data. There may be nonlinear relationships as above, dependency in the observations, known non-Gaussian data etc. One should be prepared to use models better suited to the situation, rather than torturing the data to fit a simplified modeling scheme. In case you are wondering, yes, these diagnostic plots are in fact base R graphics, and they still can look good with some work.↩ "],
["03_poly.html", "Polynomial Regression A more complex relationship", " Polynomial Regression A common application in regression to deal with nonlinear relationships involves polynomial regression. For the predictor in question, \\(x\\), we add terms e.g. quadratic (\\(x^2\\)), cubic (\\(x^3\\)) etc. to get a better fit. Consider the following data situation. Let’s fit a quadratic term and note the result. mod_poly = lm(y ~ poly(x, 2)) The R2 for this is 0.78, that’s great! But look closely and you might notice that we aren’t capturing the lower tail of this target at all, and not doing so great for observations that are high on both variables. Here’s what the data and fit would look like if we extend the data based on the underlying true function, and things only get worse. Part of the reason is that, outside of deterministic relationships due known physical or other causes, you are unlikely to discover a quadratic relationship between variables among found data. Even when it appears to fit, without a lot of data it is almost certainly overfit due to this reason. Fitting a polynomial is more akin to enforcing our vision of how the data should be, rather than letting the data speak for itself. Sure, it might be a good approximation some of the time, just as assuming a linear relationship is, but often it’s just wishful thinking. Compare the previous result to the following fit from a generalized additive model. GAMs are susceptible to extrapolation, as is every statistical model ever created. However, the original fit (in red) is much better. Notice how it was better able to follow the straightened out data points at the high end, rather than continuing the bend the quadratic enforced. A more complex relationship Perhaps you would have been satisfied with the initial quadratic fit above or perhaps a cubic fit2. We may come across a situation where the target of interest \\(y\\) is a function of some covariate \\(x\\), whose effect is not straightforward at all. In a standard regression we can generally write the model as follows: \\[y = f(x) + e\\] If we are talking a linear relationship between y and x, f(x) might be a simple sum of the covariates. \\[y = b_0 + b_1*x + e\\] If that were the case, we could do our standard regression model. Now consider the following functional form for x: \\[f(x) = sin(2(4x-2)) + 2e^{-(16^2)(x-.5)^2} + \\epsilon\\] \\[\\epsilon \\sim N(0,.3^2)\\] Let’s generate some data and take a look at it visually. set.seed(123) x = runif(500) mu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2)) y = rnorm(500, mu, .3) d = data.frame(x,y) Polynomial regression is problematic A standard linear regression is definitely not going to capture this relationship. As above, we could try and use polynomial regression here, e.g. fitting a quadratic or cubic function within the standard regression framework. However, this is unrealistic at best and at worst isn’t useful for complex relationships. In the following even with a polynomial of degree 15 the fit is fairly poor in many areas, and ‘wiggles’ in some places where there doesn’t appear to be a need to. The same would hold true for other approaches that require the functional form to be specified (e.g. so-called logistic growth curve models). It’s maybe also obvious that a target transformation (e.g. log) isn’t going to help us in this case either. In general, we’ll need tools better suited to more complex relationships, or simply ones that don’t require us to overfit/simplify the relationship we see, or guess about the form randomly until finding a decent fit. This example was actually generated with a cubic polynomial.↩ "],
["04_gam1.html", "Building up to GAMs Piecewise polynomial Introducing GAMs Polynomial spline", " Building up to GAMs Piecewise polynomial So how might we solve the problem? One way would be to divide the data into chunks at various points (knots), and fit a linear regression or polynomial model within that subset of data. The following fits a cubic polynomial for each subset of x. While this is notably better fit than e.g., a cubic polynomial, again it is unsatisfactory. The separate fits are unconnected, leading to sometimes notably different predictions for values close together. Being fit to small amounts of data means each fit overfits that area of data, leading to a fairly ‘wiggly’ result most of the time. Introducing GAMs In essence, a GAM is a GLM. What distinguishes it from the ones you know is that, unlike a standard GLM, it is composed of a sum of smooth functions of covariates instead of or in addition to the standard covariates. Consider the standard (g)lm3: \\[y = \\gamma_0 + \\gamma_1\\cdot x\\] In the above, \\(y\\) is our target, \\(x\\) the predictor variable, and \\(\\epsilon\\) the error. For the GAM, we can specify it as follows: \\[y = f(x) + \\epsilon\\] Now we are dealing with some specific (additive) function of inputs, which will not require the (possibly transformed) \\(y\\) to be a linear function of \\(x\\). It involves choosing a basis, which in technical terms means choosing a space of functions for which \\(f\\) is some element of it. On the practical side, it is a means to capture nonlinear relationships. As we’ll see later, an example would be choosing a cubic spline for the basis. Choosing a basis also means we’re selecting basis functions, which will actually go into the analysis. We can add more detail as follows: \\[y = f(x) + \\epsilon = \\sum_{j=1}^{d}B_j(x)\\gamma_j + \\epsilon\\] Above, each \\(B_j\\) is a basis function that is the transformed \\(x\\) depending on the type of basis considered, and the \\(\\gamma\\) are the corresponding regression coefficients. This might sound complicated, until you realise you’ve done this before. Let’s go back to the quadratic polynomial, which uses the polynomial basis. \\[f(x) = \\gamma_0 + \\gamma_1\\cdot x^1 \\ldots +\\gamma_d\\cdot x^d\\] In that case \\(d=2\\) and we have our standard regression with a quadratic term, but in fact, we can use this approach to produce the bases for any polynomial. As far as mechanics go, these basis functions become extra columns in the data, just like your \\(x^2\\) etc. from the polynomial approach, and then you just run a GLM! However, an additional aspect is that we will use penalized estimation, something that is quite common in some modeling contexts, and not common enough in applied research. For those new to penalized regression, again consider a standard GLM that we usually estimate with maximum likelihood, \\(l(\\beta)\\), where \\(\\beta\\) are the associated regression coefficients. Conceptually we can write the penalized likelihood as follows: \\[l_p(\\beta)= l(\\beta) - \\color{darkred}{\\mathcal{penalty}}\\] If you prefer least squares as the loss function, we can put it as: \\[\\mathcal{Loss} = \\sum (y-X\\beta)^2 + \\color{darkred}{\\mathcal{penalty}}\\] The penalty regards the complexity of the model, and specifically the size of the coefficients for the smooth terms. The practical side is that it will help to keep us from overfitting the data, where our smooth function might get too wiggly4. In summary, you’re doing a GLM, but a slightly modified one. You could have always been using penalized GLM (e.g. lasso or ridge regression), and you’d have slightly better predictive capability if you had. We can use different loss functions, different penalties etc. but the concepts are the main thing to note here. For a little more detail on this section visit the technical section. Polynomial spline Let’s get things started by demonstrating the results from a GAM that uses a polynomial spline for the basis. The brave may refer to the technical details section for additional detail. Conceptually, it is useful to continue to think of the piece-wise approach we talked about before. However, we’ll end up with a smoother and connected result when all is said and done. This is much better. We now have a connected result that isn’t overly wiggly, and more importantly, actually fits the data quite well. I am leaving out subscripts where I don’t think it helps, and as these are conceptual depictions, they usually don’t.↩ Wiggly is a highly technical term I will use throughout the presentation.↩ "],
["05_gam2.html", "Practical GAM Getting started Fitting the model", " Practical GAM One of the more powerful modeling packages in R comes with its installation5. The mixed gam computational vehicle, or mgcv package, is an extremely flexible modeling tool, and one we’ll use for our foray into generalized additive models. If you’re familiar with standard regression models, the syntax is hardly different, but adds a great many possibilities to your modeling efforts. Getting started So GAMs can be seen as a special type of GLM, that extends the model to incorporate nonlinear and other relationships, but while keeping things within a familiar framework. Behind the scenes, extra columns are added to our model matrix to do this, but their associated coefficients are penalized to help avoid overfitting. As we will see, the main syntactical difference between using gam and using glm is that you’ll use the s function to incorporate smooth terms, or classes that include basis functions as we saw before, along with a penalty. Beyond what we did previously, mgcv also incorporates a penalized regression approach. For those familiar with lasso, ridge or other penalized approaches you’ll find the same idea here. For those new to this, it’s important to add the concept and technique to your statistical modeling toolbox. Let’s look at some more interesting data than the simulated stuff from before. For the following example we’ll use the iris data set. Just kidding! We’ll look at some data regarding the Internet Movie Database (IMDB) from the ggplot2movies package. If you want to look at the original, you’ll need to install the package, and the data object within is called movies. For for information, type ?movies after loading the package. However, our example will concern yearly averages based on that data set. The primary data set is movies_yearly, with a film budget variable that has been adjusted to 2016 dollars. In addition, I’ve grouped them by whether they are an action movie or not in a separate data set movies_yearly_action. And finally, the data has been filtered to contain only those years where there a decent number of movies to go into the calculations (10 or more for yearly, 5 or more for yearly_action), and feature films (greater than 1 but less than 5 hours). load(file=&#39;data/movies_yearly.RData&#39;) Let’s examine the the time trend for ratings and budget. Size of the points represents the number of movies that year. For ratings there seems to be a definite curvilinear trend, where initially ratings for movies started high and decreased, but were on the upswing at later years. Adjusted for inflation, movie budgets don’t increase linearly over time as one might expect, and in fact were decreasing towards the end. Note that we only have adjusted budget data for 1948 on. Fitting the model Let’s fit a GAM to the yearly trend for ratings. I will also put in budget (in millions), movie length (in hours), and number of votes (in thousands) as standard linear effects. We’ll use a cubic regression spline for our basis, bs='cr', but aside from specifying the smooth term, the syntax is the same as one would use with the glm function. library(mgcv) gam_model = gam(rating ~ s(year, bs=&#39;cr&#39;) + budget_2016 + votes + length, data=movies_yearly) summary(gam_model) Family: gaussian Link function: identity Formula: rating ~ s(year, bs = &quot;cr&quot;) + budget_2016 + votes + length Parametric coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.070940 1.130976 2.715 0.00923 ** budget_2016 -0.003215 0.003259 -0.987 0.32888 votes -0.050108 0.101998 -0.491 0.62552 length 1.708533 0.713204 2.396 0.02062 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Approximate significance of smooth terms: edf Ref.df F p-value s(year) 6.892 7.885 34.94 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 R-sq.(adj) = 0.899 Deviance explained = 91.6% GCV = 0.010256 Scale est. = 0.0083302 n = 58 The first part of our output contains standard linear regression results. It is important to note that there is nothing going on here that you haven’t seen in any other GLM. For example, in this case there is no relationship between average budget and rating, whereas there is a positive relationship with average length6. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.071 1.131 2.715 0.009 budget_2016 -0.003 0.003 -0.987 0.329 votes -0.050 0.102 -0.491 0.626 length 1.709 0.713 2.396 0.021 The next part contains our information regarding the smooth terms, of which there is only one. edf Ref.df F p-value s(year) 6.892 7.885 34.943 0.000 While this suggests a statistically significant effect, it’s important to note there is not a precisely defined p-value in this setting- it clearly states ‘approximate significance’. For some more details see the technical section or my document, but we’ll briefly talk about the effective degrees of freedom, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but they are each penalized to some extent and thus the effective degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df). The thing to keep in mind is that the edf would equal 1 if the model penalized the smooth term to a simple linear relationship7, and so the effective degrees of freedom falls somewhere between 1 and k-1, where k is chosen based on the basis. You can think of it as akin to the number of knots. The default here was 10. Basically if it’s 1, just leave it as a linear component, or if it’s close to k, maybe bump up the default to allow for more wiggliness. Visualizing the effects While the statistical result is helpful to some extent, the best way to interpret smooth terms is visually. The ability to do so is easy, you can just use the plot method on the model object. However, the default plot may be difficult to grasp at first. It is a component plot, which plots the original variable against the linear combination of its basis functions, i.e. the sum of each basis function multiplied by its respective coefficient. For example, if we were using a quadratic polynomial, it would be the plot of \\(x\\) against \\(b_1\\cdot x + b_2\\cdot x^2\\). For GAMs, \\(y\\) is also centered, and what you end up with is something like the following. plot(gam_model) So this tells us the contribution of the year to the model fit8. For an alternative, consider the visreg package, which focuses on standard predicted values holding other variables constant, and it just looks slightly nicer by default while allowing for more control. library(visreg) visreg(gam_model, xvar=&#39;year&#39;, partial=F) You of course can look at predictions at any relevant values for the covariates, as you can with any other model. Consider the following effect of time while keeping budget and length constant at their means, and votes at 10009. movies_yearly %&gt;% na.omit() %&gt;% transmute(budget_2016 = mean(budget_2016), length = mean(length), votes = 1, year=year) %&gt;% add_predictions(gam_model, var=&#39;Predicted_Rating&#39;) %&gt;% plot_ly() %&gt;% add_lines(~year, ~Predicted_Rating) See the technical details on how to reproduce both the mgcv and visreg default plots. In the end, controlling for budget, length, and popularity, movie quality decreased notably until the 70s, and remained at that level until the late 90s, when quality increased even more dramatically. For reasons I’ve not understood, other packages still depend on or extend the gam package, which doesn’t possess anywhere near the functionality (though the authors literally wrote the book on GAM), or even the splines package, which comes with base R but has even less. If you’re going to use additive models, I suggest starting with mgcv until you know it cannot do what you want, then consider the extensions to it that are out there.↩ Presumably we have to justify the time spent watching longer movies. But remember these are averages, so bumping up the average length for a given year would undoubtedly fundamentally change the types of movies considered.↩ You can actually penalize the covariate right out of the model if desired, i.e. edf=0.↩ This is the same as what the base R function termplot would provide for a standard (g)lm. You can request the plots be applied to the non-smooth terms in the model as well. And the residuals function has an argument, type='partial' that can be applied to produce the partial residuals, which are the residuals + term. See ?termplot for more info, and the technical details section. Note that in the case of a single smooth term and nothing else in the model, one could add the intercept to get back to the response scale. However, the visreg approach would be the way to go in that case, as it extends to easy set values for other covariates as well.↩ If we had put votes at its median, you would basically duplicate the previous plot.↩ "],
["06_issues.html", "Issues", " Issues There are some complexities associated with GAMs that will take some getting used to. Choice of distribution: With mgcv you have more options than the standard GLM, i.e. it generalizes beyond the exponential family of distributions, so put some thought into what you think might best represent the data generating process. Choice of smooth: Some might be particularly appropriate to a given situation (e.g. spatial). In many situations you’ll have a choice. They should not lead to fundamentally different conclusions though. As an example, here’s the result of the same model with several different smooths with the default k. See ?smooth.terms for the choices available. Choosing whether to keep an effect linear: Sometimes it may look like the effect isn’t nonlinear or much more more than linear, and a choice will have to be made whether to go the simpler route. If k=1, this is straightforward. If not exactly, you might have to make a choice. Diagnostics: There are some additional diagnostics to consider, such as the k parameter. Concurvity, which is collinearity for the GAM setting, also comes into play. Model comparison: In general one can use AIC as with standard GLMs, and in addition one has the GCV statistic. However, there are special considerations for an ANOVA style approach, so see ?anova.gam for details. Fuzzy p-values: For folks that are used to complex models, e.g. penalized regression, mixed models, machine learning techniques, this is not new. For those new to such models, you must unlearn what you have learned. Areas of little data: The fit can become highly variable when data is sparse. The standard errors of the plots will be very wide in those areas, and you should avoid going too far with any conclusions drawn for those data points. Just like with any new statistical modeling technique, diving in with something you’re already familiar with is a good way to start getting comfortable. One way to do so would be to revisit an analysis you’ve done in the past and re-analyze it with a GAM. And don’t forget, you also have an entire text devoted to the package for as much detail as you want, and the help files are quite detailed as well. "],
["07_extensions.html", "Extensions Extensions Connections &amp; Generalizations", " Extensions Extensions More bases There are many options available in mgcv beyond cubic splines, including thin plate, duchon, p-splines and more, as well as ones for cyclical effects, geographical, and beyond. See ?smooth.terms for more information. Interactions Not only can we have multiple smooth terms in a model, we can implement interactions of smooth terms. The following shows a model in which we could combine the effects of year and length of the movie via the te function, along with the corresponding predictive plots. I use the raw movies data set, with some filters to keep the observations retained more comparable. Note that the 3d is interactive, so twist it however you like to see what’s going on. gam_model_2d = gam(rating ~ te(year, length), data=movies_filtered) We now get the sense that the highest expected ratings might be expected with more longer films, perhaps even more for more recent ones. Starting around the 60s, shorter films are associated with worse ratings. There is also a way to do this interaction in a manner similar to ANOVA decomposition with both main effects and interaction. See the ti function. Grouped Effects We can apply a smooth for a continuous covariate across levels of some grouping factor, which extends our interactions to categorical variables. In the following, we’ll let year have a different effect for Action movies vs. other movies. gam_model_by = gam(rating ~ s(year, by=Action) + Action + budget_2016 + votes + length, data=movies_yearly_action) In this case Action movies are generally rated worse, and suffered a second and sharper dip starting in the 80s10. Also, their recent rise is not quite as sharp. Another alternative to this type of grouped approach, which would be applicable if there are many categories, specifies a specific type of smooth that would be similar to random slopes in a mixed model (see ?factor.smooth.interaction). We now turn to the mixed model approach with GAMs. Mixed Models Consider the matrix \\(Z\\) that includes the basis functions, with associated coefficients \\(\\gamma\\), we could then write our model in matrix form as follows: \\[ y = X\\beta + Z\\gamma + \\epsilon\\] If you’ve spent much time with mixed models, that probably looks familiar to you. It not only looks like the mixed model, we can exploit this to produce the same results. The mgcv package in fact has many tools specifically for mixed models. For this example I will use the sleep study data that comes with the lme4 package. It has reaction times for some tests where 18 subjects were restricted to 3 hours of sleep each night for 10 days. We fit a model with (uncorrelated) random intercepts and random slopes for Days. library(lme4) mod_lme4 = lmer(Reaction ~ Days + (1|Subject) + (0 + Days|Subject), sleepstudy) mod_gamm = gam(Reaction ~ Days + s(Subject, bs=&#39;re&#39;) + s(Days, Subject, bs=&#39;re&#39;), data=sleepstudy, method=&#39;REML&#39;) VarCorr(mod_lme4) Groups Name Std.Dev. Subject (Intercept) 25.0513 Subject.1 Days 5.9882 Residual 25.5653 gam.vcomp(mod_gamm) Standard deviations and 0.95 confidence intervals: std.dev lower upper s(Subject) 25.051370 16.085365 39.015038 s(Days,Subject) 5.988153 4.025248 8.908263 scale 25.565254 22.791745 28.676269 Rank: 3/3 The results are essentially identical. The mgcv package can also use lme4 and nlme under the hood via other functions (gamm4 package and gamm function respectively). Extending GAMs into the mixed model world opens the door to some very powerful modeling possibilities. Spatial models Consider a data set with latitude and longitude coordinates among other covariates used to model some target variable. A spatial regression analysis uses an approach to account for spatial covariance among the observation points (e.g. kriging). Such an approach is a special case of Gaussian process which, as we note later, GAMs are as well. As such we can add spatial models to the sorts of models covered by GAMs too. The following gives a sense of the syntax. The mgcv package even has a 'gp' smooth. gam(y ~ s(lat, lon, bs=&#39;gp&#39;)) # Matern by default What about the discrete case, where the spatial random effect is based on geographical regions? This involves a penalty that is based on the adjacency matrix of the regions, where if there are \\(g\\) regions, the adjacency matrix is a \\(g \\times g\\) indicator matrix where there is some non-zero value when region i is connected to region j, and 0 otherwise. In addition an approach similar to that for a random effect is used to incorporate observations belonging to specific regions. These are sometimes referred to as geoadditive models. You’ll be shocked to know that mgcv has a smooth construct for this situation as well, bs='mrf', where mrf stands for Markov random field, which is an undirected graph. Plotting the smooth term is akin to displaying the spatial random effect. The following shows example code. See the help for mrf for more detail. gam(crime ~ s(region, bs = &quot;mrf&quot;, xt = polygonListObject), data = df, method = &quot;REML&quot;) My God! It’s full of STARs! The incorporation of additive, mixed, and spatial regression models all within one modeling framework gets us to STARs, or, structured additive regression models. This is an extremely general and powerful modeling tool that can take you very, very far11. Connections &amp; Generalizations Adaptive Basis Function Models GAMs can be seen as belonging to a family of what are called adaptive basis function models. These include random forests, neural nets, and boosting approaches. GAMs might be seen as a midpoint lying between highly interpretable standard linear models and those more black box methods. Beyond GLM family distributions The mgcv package provides a plethora of distributions to use in modeling that might useful including: tweedie negative binomial ordered categorical beta t zero-inflated Poisson (ZIP) cox proportional hazards gaulss Gaussian model for both mean (location) and standard deviation (scale) gevlss same for generalized extreme value ziplss for a hurdle approach to zero-inflation multivariate normal multinomial logistic Extending Random Effects In addition to distribution extensions that could also be incorporated with random effects, providing modeling capabilities not generally offered in mixed model packages, mgcv also allows for correlated residuals structure as with nlme, but for the generalized case. This is especially of interest in the longitudinal setting, where one expects residuals for observations closer in time to be more correlated. In addition, the distributions just noted can also be used for in the random effects setting, e.g. a mixed model with beta distributed response. Gaussian Processes Where the Gaussian distribution is over vectors and defined by a mean vector and covariance matrix, a Gaussian Process is a distribution over functions. A function \\(f\\) is distributed as a Gaussian Process defined by a mean function \\(m\\) and covariance function \\(k\\)12. \\[f\\sim \\mathcal{GP}(m,k)\\] It turns out that GAMs with a thin plate, tensor product, or cubic spline smooth are maximum a posteriori (MAP) estimates of Gaussian processes with specific covariance functions and a zero mean function. In that sense one might segue nicely to Gaussian processes if familiar with additive models. The mgcv package, … wait for it…, also allows one to use a spline form of Gaussian process bs='gp'. Bayesian Bayesian GAM are also possible within R through a variety of packages. The brms package can take your GAM to the Bayesian setting using the same syntax even, and is particularly useful for adding the mixed model component as well, and alternative distributions. It is based on the Stan programming language. This suggest that the IMDB ratings might be a bit problematic. 1979-1989 was practically the golden age for Action movies, with things like Rambo, Alien, Terminator, Mad Max, Die Hard, Indiana Jones, RoboCop, Bloodsport, the Killer, The Untouchables, Police Story, Escape from New York, Predator, Lethal Weapon etc. Surely there couldn’t have been so many bad movies as to outweigh those? Oh, wait…↩ In case you aren’t familiar with the phrase ‘it’s full of stars’, it comes from the book 2001: A Space Odyssey: see link. The film adaptation is considered one of the greatest films of all time, because it is. The quote is not in that movie, but in the film sequel 2010.↩ They have a close tie to reproducing kernel hilbert space methods, and generalize commonly used models in spatial modeling (kriging). In addition, a GP can be interpreted as a standard multilayer perceptron neural net with a single hidden layer consisting of an infinite number of nodes.↩ "],
["08_summary.html", "Summary", " Summary The purpose of this workshop was to provide awareness of a powerful modeling technique, that is as easy to implement as a standard GLM. The additional complexity will take some getting used to just like any new tool would, but the payoff is worth it in my opinion. Hope you have some fun adding some wiggle to your model! "],
["09_exercises.html", "Exercises Exercise 1 Exercise 2 Exercise 3 Exercise 4", " Exercises Exercise 1 Let’s start off with a single covariate. We’ll use the motorcycle data set, which is a data frame giving a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets. times: in milliseconds after impact accel: acceleration in g data(&#39;mcycle&#39;, package=&#39;MASS&#39;) Run a GAM predicting head acceleration by time with time as a smooth term. You can plot the result with visreg as follows. If you haven’t installed it, you’ll need to do so. Otherwise just use the plot function. library(visreg) visreg(my_gam_model, partial=F) Exercise 2 Next we’ll use a the yearly movie data consisting of the following variables. year: Year of release. budget_2016: Average budget in millions 2016 US dollars length: Average length in minutes. rating: Average IMDB user rating. votes: Average number of IMDB users who rated movies that year, in thousands. N: The number of movies that went into the averaging. 2a Run an lm/glm with budget_2016 as the target and your choice of covariates. Here is some starter code. load(&#39;data/movies_yearly.RData&#39;) mod_glm = glm(budget_2016 ~ x + z, data=movies_yearly) 2b Run an gam with the same model but no smooth terms (i.e. just change the function to gam) 2c Now run a gam with any smooth terms you care to. What is your interpretation of the results? Exercise 3 Use the movies_yearly_action data. This data is the same as the yearly data, but broken out into whether the movie was categorized as being and Action flick (possibly in combination of another genre) or Other. Run a GAM where the outcome is budget in millions of 2016 dollars. # If you haven&#39;t already, load the data as in exercise 2 gam_model_byAction = gam(budget_2016 ~ s(year, by=Action) + Action + votes + length, data=movies_yearly_action) Plot the result with visreg as follows. visreg::visreg(gam_model_byAction, xvar=&#39;year&#39;, by=&#39;Action&#39;, overlay=T, partial=F) What trends do you see? Is the yearly trend the same for action movies vs other movies? Do you think both trends should be allowed to ‘wiggle’? Exercise 4 Keep it local with the following Great Lakes data set13. The following model uses a Gaussian process smooth for year and a p-spline for the monthly effect, both by the corresponding lake. Try increasing k for the year effect, and with bs='cp' (a cyclic p-spline) for the month effect. Do you think using the cyclic option, where the endpoints are equal, was viable approach? load(&#39;data/greatLakes.RData&#39;) gam_monthly_cycle = gam(meters*10 ~ lake + s(year, by=lake, bs=&#39;gp&#39;, k=10) + s(month_num, bs=&#39;ps&#39;, by=lake), data=lakes) # summary(gam_monthly_cycle) monthpd = visreg(gam_monthly_cycle, xvar=&#39;month_num&#39;, by=&#39;lake&#39;, overlay=T, plot=F)$fit yearpd = visreg(gam_monthly_cycle, xvar=&#39;year&#39;, by=&#39;lake&#39;, overlay=T, plot=F)$fit monthpd %&gt;% ggplot() + geom_line(aes(x=month_num, y=visregFit)) + scale_x_continuous(breaks=1:12, labels=month.abb) + facet_wrap(~lake, scales=&#39;free_y&#39;) + theme(axis.text.x = element_text(angle=-45, hjust = 0)) yearpd %&gt;% ggplot() + geom_line(aes(x=year, y=visregFit)) + facet_wrap(~lake, scales=&#39;free_y&#39;) Lake data from http://www.lre.usace.army.mil/Missions/Great-Lakes-Information/Great-Lakes-Water-Levels/Historical-Data/. Note that in the exercise we model the water level in decimeters.↩ "],
["10_Technical.html", "Technical details GAM Introduction A detailed example The number of knots and where to put them. Interpreting output for smooth terms", " Technical details Workshops see all types of folks at different levels. This is for some of the more technically inclined, though you’ll find more context and detail in my document and Wood (2006), from which a good chunk of this section is taken more or less directly from. GAM Introduction A GAM is a GLM whose linear predictor includes a sum of smooth functions of covariates. With link function \\(g(.)\\), model matrix \\(X\\) of \\(n\\) rows and \\(p\\) covariates (plus a column for the intercept), a vector of \\(p\\) coefficients \\(\\beta\\), we can write a GLM as follows: \\[\\mathrm{GLM:}\\quad g(\\mu) = X\\beta\\] For the GAM, it could look something like: \\[\\mathrm{GAM:}\\quad g(\\mu) = X\\beta + f(x_1) + f(x_2) + f(x_3, x_4) + \\ldots\\] or as we did when discussing mixed models: \\[\\mathrm{GAM:}\\quad g(\\mu) = X\\beta + Z\\gamma\\] Where \\(Z\\) represents the basis functions of some subset of \\(X\\) or other covariates. Thus we can have any number of smooth terms, possibly of different bases, and even combinations of them. Finally we can depict the structured additive regression, or STAR, model as a GAM + random effects (\\(\\Upsilon\\)) + spatial effects (\\(\\Xi\\)) + other fun stuff. \\[\\mathrm{STAR:}\\quad g(\\mu) = X\\beta + Z\\gamma + \\Upsilon\\varpi + \\Xi\\varrho + \\ldots \\mathbf{?}\\vartheta \\] Penalized regression Consider a standard GLM that we usually estimate with maximum likelihood, \\(l(\\beta)\\), where \\(\\beta\\) are the associated regression coefficients. We can write the penalized likelihood as follows: \\[l_p(\\beta)= l(\\beta) - \\color{darkred}{\\lambda B&#39;SB}\\] Where \\(S\\) is a penalty matrix of known coefficients14. If you prefer least squares as the loss function, we can put it as: \\[\\mathcal{Loss} = \\sum (y-X\\beta)^2 + \\color{darkred}{\\lambda B&#39;SB}\\] So if we’re maximizing the likelihood will make it lower than it otherwise would have been, and vice versa in terms of a loss function. What it means for a GAM is that we’ll add a penalty for the coefficients associated with the basis functions15. The practical side is that it will help to keep us from overfitting the data, where our smooth function might get too wiggly. As \\(\\lambda \\rightarrow \\infty\\), the result is a linear fit because any wiggliness will add too much to the loss function. As \\(\\lambda \\rightarrow 0\\), we have the opposite effect, where any wiggliness is incorporated into the model. In mgcv, by default the estimated parameters are chosen via a generalized cross validation, or GCV, approach, and that statistic is reported in the summary. It modifies the loss function depicted above to approximate leave-one-out cross-validation selection. A detailed example The following will demonstrate a polynomial spline by hand16. The example, and in particular the visual display, is based on a depiction in Fahrmeier et al. (2013). The approach is defined as follows, with \\(\\kappa\\) knots on the interval \\([a,b]\\) as \\(a=\\kappa_1 &lt; ... &lt; \\kappa_m =b\\): \\[y_i = \\gamma_1 + \\gamma_2x_i + ... + \\gamma_{l+1}(x_i)_+^l + \\gamma_{l+2}(x_i - \\kappa_2)_+^l ... + \\gamma_{l+m-1}(x_i - \\kappa_{m-1})_+^l + e_i\\] \\[(x - \\kappa_j)^l_+= \\begin{cases} (x-\\kappa_j)^l &amp; x \\geq \\kappa_j \\\\ 0 &amp; \\textrm{otherwise} \\end{cases}\\] So we subtract the current knot being considered from \\(x\\) for values of \\(x\\) greater than or equal to the knot, otherwise, it’s 0. It might look complicated, but note that there is nothing particularly special about the model itself. It is just a standard linear regression model when everything is said and done. More generally we can write a GAM as follows: \\[y = f(x) + e = \\sum_{j=1}^{d}B_j(x)\\gamma_j + e\\] With the spline above this becomes: \\[B_1(x)=1, B_2(x)=x, ..., B_{l+1}(x)=x^l, B_{l+2}(x)=(x-\\kappa_2)_+^l...B_{d}(x)=(x-\\kappa_{m-1})_+^l\\] Let’s see it in action. Here our polynomial spline will be done with degree \\(l\\) equal to 1, which means that we are just fitting a linear regression between knots. This uses the data we employed for demonstration before. knots = seq(0, 1, by=.1) knots = knots[-length(knots)] # don&#39;t need the last value l = 1 bs = sapply(1:length(knots), function(k) ifelse(x &gt;= knots[k], (x-knots[k])^l, 0)) head(bs) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 0.2875775 0.1875775 0.08757752 0.0000000 0.000000000 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 [2,] 0.7883051 0.6883051 0.58830514 0.4883051 0.388305135 0.2883051 0.1883051 0.08830514 0.0000000 0.00000000 [3,] 0.4089769 0.3089769 0.20897692 0.1089769 0.008976922 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 [4,] 0.8830174 0.7830174 0.68301740 0.5830174 0.483017404 0.3830174 0.2830174 0.18301740 0.0830174 0.00000000 [5,] 0.9404673 0.8404673 0.74046728 0.6404673 0.540467284 0.4404673 0.3404673 0.24046728 0.1404673 0.04046728 [6,] 0.0455565 0.0000000 0.00000000 0.0000000 0.000000000 0.0000000 0.0000000 0.00000000 0.0000000 0.00000000 # a more digestible example to make things even more clear # x2 = seq(0,1, length.out=30) # sapply(1:3, function(k) ifelse(x2 &gt;= knots[k], (x2-knots[k])^l, 0)) If we plot this against our target variable \\(y\\), it doesn’t look like much. If we multiply each basis by it’s corresponding regression coefficient we can start to interpret the result. lmMod = lm(y~.-1, bs) # just a regression! bscoefs = coef(lmMod) bsScaled = sweep(bs, 2, bscoefs,`*`) colnames(bsScaled) = c(&#39;int&#39;, paste0(&#39;X&#39;, 1:10)) round(bscoefs, 3) int X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 0.783 -6.637 -1.388 3.839 7.366 25.769 -41.462 15.167 -7.144 -1.738 -3.394 In the plot above, the initial dot represents the global constant (\\(\\gamma_1\\), i.e. our intercept). We have a decreasing function starting from that point onward (line). Between .1 and .2 (line), the coefficient is negative again, furthering the already decreasing slope (i.e. steeper downward). The fourth coefficient is positive, which means that between .2 and .3 our decreasing trend is lessening (line). Thus our coefficients \\(j\\) tell us the change in slope for the data from the previous section of data defined by the knots. The length’s of the lines reflect the size of the coefficient, i.e. how dramatic the change is. If this gets you to thinking about interactions in more common model settings, you’re on the right track (e.g. adding a quadratic term is just letting x have an interaction with itself; same thing is going on here). Finally if we plot the sum of the basis functions, which is the same as taking our fitted values from a regression on the basis expansion of X, we get the following fit to the data. And we can see the trend our previous plot suggested. One of the more common approaches with GAMs uses a cubic spline fit. So we’ll change our polynomial degree from 1 to 3 (i.e. l = 3). Now we’re getting somewhere. Let’s compare it to the gam function from the mgcv package. We won’t usually specify the knots directly, and even as we have set things up similar to the mgcv approach, the gam function is still doing some things our by-hand approach is not (penalized regression). We still get pretty close agreement however. We can see that we’re on the right track by using the constructor function within mgcv and a custom function for truncated power series like what we’re using above. See the example in the help file for smooth.construct for the underlying truncated power series function. I only show a few of the columns, but our by-hand construction and that used by gam are identical. xs = scale(x, scale=F) bs = sapply(1:length(knots), function(k) ifelse(x &gt;= knots[k], (x-knots[k])^l, 0)) sm = smoothCon(s(x, bs=&#39;tr&#39;, k=14), data=d, knots=list(x=knots))[[1]] head(sm$X[,1:6]) [,1] [,2] [,3] [,4] [,5] [,6] [1,] 1 -0.20770617 0.043141852 -0.0089608288 2.378290e-02 0.006599976 [2,] 1 0.29302145 0.085861568 0.0251592810 4.898725e-01 0.326094166 [3,] 1 -0.08630677 0.007448858 -0.0006428868 6.840635e-02 0.029497019 [4,] 1 0.38773372 0.150337434 0.0582908920 6.885061e-01 0.480080698 [5,] 1 0.44518360 0.198188434 0.0882302397 8.318233e-01 0.593693698 [6,] 1 -0.44972719 0.202254545 -0.0909593678 9.454771e-05 0.000000000 modelMatrix = cbind(1, xs, xs^2, xs^3, bs) all.equal(sm$X, modelMatrix) [1] TRUE Preview of other bases As an example of other types of smooth terms we might use, here are the basis functions for b-splines. They work notably differently, e.g. over intervals of \\(l+2\\) knots. The number of knots and where to put them. A natural question may arise as to how many knots to use. More knots potentially means more ‘wiggliness’, as demonstrated here. Note that these are number of knots, not powers in a polynomial regression as shown in the main section of this document. However, as we’ll come to learn in the next section, we don’t really have to worry about this except in the conceptual sense, i.e. being able to control the wiggliness. The odds of you knowing beforehand the number of knots and where to put them is somewhere between slim and none, so this is a good thing. Interpreting output for smooth terms Effective degrees of freedom In interpreting the output from mgcv, we’ll start with the effective degrees of freedom, or edf. In typical OLS regression the model degrees of freedom is equivalent to the number of predictors/terms in the model. This is not so straightforward with a GAM due to the smoothing process and the penalized regression estimation procedure. In this example there are actually 9 terms associated with this smooth, but their corresponding parameters are each penalized to some extent and thus the effective degrees of freedom does not equal 9. For hypothesis testing an alternate edf is actually used, which is the other one provided there in the summary result (Ref.df). For more on this see my document, ?summary.gam and ?anova.gam. At this point you might be thinking these p-values are a bit fuzzy, and you’d be right. As is the case with mixed models, machine learning approaches, etc., p-values are not straightforward. The gist is that in the GAM setting they aren’t to be used for harsh cutoffs, say, at an arbitrary .05 level, but then standard p-values shouldn’t be used that way either. If they are pretty low you can feel comfortable claiming statistical significance, but if you want a tight p-value you’ll need to go back to using a GLM. The edf would equal 1 if the model penalized the smooth term to a simple linear relationship17, and so the effective degrees of freedom falls somewhere between 1 and k-1 (or k), where k is chosen based on the basis. You can think of it as akin to the number of knots. There is functionality to choose the k value, but note the following from Wood in the help file for ?choose.k: So, exact choice of k is not generally critical: it should be chosen to be large enough that you are reasonably sure of having enough degrees of freedom to represent the underlying ‘truth’ reasonably well, but small enough to maintain reasonable computational efficiency. Clearly ‘large’ and ‘small’ are dependent on the particular problem being addressed. And the following: One scenario that can cause confusion is this: a model is fitted with k=10 for a smooth term, and the EDF for the term is estimated as 7.6, some way below the maximum of 9. The model is then refitted with k=20 and the EDF increases to 8.7 - what is happening - how come the EDF was not 8.7 the first time around? The explanation is that the function space with k=20 contains a larger subspace of functions with EDF 8.7 than did the function space with k=10: one of the functions in this larger subspace fits the data a little better than did any function in the smaller subspace. These subtleties seldom have much impact on the statistical conclusions to be drawn from a model fit, however. If you want a more statistically oriented approach, see ?gam.check. Deviance explained R-sq.(adj) = 0.904 Deviance explained = 92.1% GCV = 0.010074 Scale est. = 0.0081687 n = 58 For the standard Gaussian setting, we can use our R2. Also provided is ‘deviance explained’, which in this setting is identical to the unadjusted R2, but for non-Gaussian families would be preferred. As noted above, the GCV, or generalized cross validation score, can be taken as an estimate of the mean square prediction error based on a leave-one-out cross validation estimation process. Steps can be taken to choose model parameters specifically based on this (it’s actually the default, as opposed to, e.g. maximum likelihood). Visual depiction The following reproduces the plots produced by mgcv and visreg. I’ll even use base R plotting to ‘keep it real’. First we’ll start with the basic GAM plot. First we get the term for year, i.e. the linear combination of the basis functions for year. The mgcv package provides this for you via the predict function with type='terms'. For non-smooth components this is just the original covariate times its corresponding coefficient. Next we need the partial residuals, which are just the basic residuals plus the term of interest added. With the original predictor variable, we’re ready to proceed. year_term = predict(gam_model, type=&#39;terms&#39;)[,&#39;s(year)&#39;] res_partial = residuals(gam_model) + year_term year = 1948:2005 par(mfrow=c(1,2)) plot(year, res_partial, ylim=c(-.8,.8), col=&#39;black&#39;, ylab=&#39;s(year, 7.06)&#39;, pch=19, main=&#39;ours&#39;) lines(year, year_term) plot(gam_model, se=F, residuals=T, pch=19, rug=F, ylim=c(-.8,.8), main=&#39;mgcv&#39;) Now for visreg. It uses the underlying predict function also, but gets a standard prediction while holding the other variables at key values (which you can manipulate). By default these key values are at the median for numeric variables, and most common category for categorical variables. pred_data = movies_yearly %&gt;% select(budget_2016, votes, length) %&gt;% na.omit() %&gt;% summarise_all(median) %&gt;% data.frame(year=1948:2005) preds = predict(gam_model, newdata=pred_data) res_partial = gam_model$residuals + preds par(mfrow=c(1,2)) plot(year, preds, col=&#39;dodgerblue&#39;, ylab=&#39;f(year)&#39;, lwd=3, ylim=c(5,7), type=&#39;l&#39;, main=&#39;ours&#39;) points(year, res_partial, col=&#39;gray50&#39;, pch=19) visreg(gam_model, xvar=&#39;year&#39;, points.par=list(cex=1), band=F, ylim=c(5,7), main=&#39;visreg&#39;) For example, it would be an identity matrix if we were dealing with random effects, or a neighborhood matrix when dealing with a spatial context.↩ More formally, the penalty focuses on the second derivative of the function (i.e. it’s curvature): \\(\\lambda \\int f&#39;&#39;(x)^2dx\\)↩ This is the truncated power series variant of polynomial splines.↩ You can actually penalize the covariate right out of the model if desired, i.e. edf=0.↩ "],
["100_references.html", "References", " References My doc on GAMs. Wood, S. N. 2006. Generalized Additive Models: An Introduction with R. Fahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian Marx. 2013. Regression: Models, Methods and Applications. (electronic access through UM library) "]
]
